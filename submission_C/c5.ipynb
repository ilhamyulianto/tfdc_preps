{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function downloads and extracts the dataset to the directory that contains this file.\n",
    "# DO NOT CHANGE THIS CODE\n",
    "# (unless you need to change the URL)\n",
    "def download_and_extract_data():\n",
    "    url = 'https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/household_power.zip'\n",
    "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
    "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "\n",
    "\n",
    "# This function normalizes the dataset using min max scaling.\n",
    "# DO NOT CHANGE THIS CODE\n",
    "def normalize_series(data, min, max):\n",
    "    data = data - min\n",
    "    data = data / max\n",
    "    return data\n",
    "\n",
    "# COMPLETE THE CODE IN THE FOLLOWING FUNCTION.\n",
    "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
    "    # YOUR CODE HERE\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    dataset = dataset.window(n_past + n_future, shift=shift, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch (n_past + n_future))\n",
    "    dataset = dataset.shuffle(1000)\n",
    "    dataset = dataset.map(lambda window: (window[:-n_future], window[-n_future:, :1]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset # YOUR CODE HERE\n",
    "\n",
    "# COMPLETE THE CODE IN THE FOLLOWING FUNCTION.\n",
    "def solution_C5():\n",
    "    # Downloads and extracts the dataset to the directory that contains this file.\n",
    "    download_and_extract_data()\n",
    "    # Reads the dataset from the csv.\n",
    "    df = pd.read_csv('household_power_consumption.csv', sep=',',\n",
    "                     infer_datetime_format=True, index_col='datetime', header=0)\n",
    "\n",
    "    # Number of features in the dataset. We use all features as predictors to\n",
    "    # predict all features at future time steps.\n",
    "    N_FEATURES = df.shape[1] # YOUR CODE HERE\n",
    "\n",
    "    # Normalizes the data\n",
    "    # DO NOT CHANGE THIS\n",
    "    data = df.values\n",
    "    split_time = int(len(data) * 0.5)\n",
    "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
    "\n",
    "    # Splits the data into training and validation sets.\n",
    "    x_train = data[:split_time] # YOUR CODE HERE\n",
    "    x_valid = data[split_time:] # YOUR CODE HERE\n",
    "\n",
    "    # DO NOT CHANGE THIS\n",
    "    BATCH_SIZE = 32\n",
    "    N_PAST = 24 # Number of past time steps based on which future observations should be predicted\n",
    "    N_FUTURE = 24  # Number of future time steps which are to be predicted.\n",
    "    SHIFT = 1  # By how many positions the window slides to create a new window of observations.\n",
    "\n",
    "    # Code to create windowed train and validation datasets.\n",
    "    # Complete the code in windowed_dataset.\n",
    "    train_set = windowed_dataset(x_train, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT) # YOUR CODE HERE\n",
    "    valid_set = windowed_dataset(x_valid, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT) # YOUR CODE HERE\n",
    "\n",
    "    # Code to define your model.\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Whatever your first layer is, the input shape will be (N_PAST = 24, N_FEATURES = 7)\n",
    "        # YOUR CODE HERE\n",
    "        tf.keras.layers.LSTM(50, activation='relu', input_shape=[N_PAST, N_FEATURES]),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(50, activation='relu'),\n",
    "        tf.keras.layers.Dense(N_FUTURE),\n",
    "    ])\n",
    "\n",
    "    # Code to train and compile the model\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9), loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "\n",
    "    model.fit(train_set, validation_data=valid_set, epochs=10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilham\\AppData\\Local\\Temp\\ipykernel_72104\\825146727.py:34: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df = pd.read_csv('household_power_consumption.csv', sep=',',\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1349/1349 [==============================] - 11s 7ms/step - loss: 0.0089 - mae: 0.1023 - val_loss: 0.0057 - val_mae: 0.0807\n",
      "Epoch 2/10\n",
      "1349/1349 [==============================] - 9s 7ms/step - loss: 0.0056 - mae: 0.0779 - val_loss: 0.0044 - val_mae: 0.0655\n",
      "Epoch 3/10\n",
      "1349/1349 [==============================] - 9s 7ms/step - loss: 0.0047 - mae: 0.0665 - val_loss: 0.0040 - val_mae: 0.0608\n",
      "Epoch 4/10\n",
      "1349/1349 [==============================] - 11s 8ms/step - loss: 0.0044 - mae: 0.0621 - val_loss: 0.0039 - val_mae: 0.0585\n",
      "Epoch 5/10\n",
      "1349/1349 [==============================] - 12s 9ms/step - loss: 0.0042 - mae: 0.0599 - val_loss: 0.0038 - val_mae: 0.0577\n",
      "Epoch 6/10\n",
      "1349/1349 [==============================] - 12s 9ms/step - loss: 0.0042 - mae: 0.0588 - val_loss: 0.0038 - val_mae: 0.0568\n",
      "Epoch 7/10\n",
      "1349/1349 [==============================] - 12s 8ms/step - loss: 0.0041 - mae: 0.0578 - val_loss: 0.0037 - val_mae: 0.0565\n",
      "Epoch 8/10\n",
      "1349/1349 [==============================] - 11s 8ms/step - loss: 0.0040 - mae: 0.0571 - val_loss: 0.0037 - val_mae: 0.0553\n",
      "Epoch 9/10\n",
      "1349/1349 [==============================] - 11s 8ms/step - loss: 0.0040 - mae: 0.0565 - val_loss: 0.0037 - val_mae: 0.0562\n",
      "Epoch 10/10\n",
      "1349/1349 [==============================] - 9s 7ms/step - loss: 0.0040 - mae: 0.0562 - val_loss: 0.0037 - val_mae: 0.0549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilham\\miniconda3\\envs\\vision1\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# The code below is to save your model as a .h5 file.\n",
    "# It will be saved automatically in your Submission folder.\n",
    "if __name__ == '__main__':\n",
    "    # DO NOT CHANGE THIS CODE\n",
    "    model = solution_C5()\n",
    "    model.save(\"model_C5.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
