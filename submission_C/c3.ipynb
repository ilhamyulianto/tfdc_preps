{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution_C3():\n",
    "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/cats_and_dogs.zip'\n",
    "    urllib.request.urlretrieve(data_url, 'cats_and_dogs.zip')\n",
    "    local_file = 'cats_and_dogs.zip'\n",
    "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
    "    zip_ref.extractall('data/')\n",
    "    zip_ref.close()\n",
    "\n",
    "    BASE_DIR = 'data/cats_and_dogs_filtered'\n",
    "    train_dir = os.path.join(BASE_DIR, 'train')\n",
    "    validation_dir = os.path.join(BASE_DIR, 'validation')\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255.0,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    ) # YOUR CODE HERE\n",
    "\n",
    "    # YOUR IMAGE SIZE SHOULD BE 150x150\n",
    "    # Make sure you used \"binary\"\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary'\n",
    "    ) # YOUR CODE HERE\n",
    "\n",
    "    validation_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # YOUR CODE HERE, end with a Neuron Dense, activated by 'sigmoid'\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=RMSprop(learning_rate=0.0001),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=30,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=50\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 31s 303ms/step - loss: 0.6956 - accuracy: 0.4990 - val_loss: 0.6879 - val_accuracy: 0.5000\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 0.6895 - accuracy: 0.5235 - val_loss: 0.6746 - val_accuracy: 0.5990\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 0.6782 - accuracy: 0.5660 - val_loss: 0.6584 - val_accuracy: 0.5970\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 0.6728 - accuracy: 0.5610 - val_loss: 0.6493 - val_accuracy: 0.6140\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 0.6678 - accuracy: 0.5675 - val_loss: 0.6386 - val_accuracy: 0.6200\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 18s 178ms/step - loss: 0.6549 - accuracy: 0.5905 - val_loss: 0.6294 - val_accuracy: 0.6320\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 0.6500 - accuracy: 0.6070 - val_loss: 0.6236 - val_accuracy: 0.6420\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.6404 - accuracy: 0.6380 - val_loss: 0.5928 - val_accuracy: 0.7030\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.6286 - accuracy: 0.6415 - val_loss: 0.5783 - val_accuracy: 0.6960\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 18s 174ms/step - loss: 0.6105 - accuracy: 0.6545 - val_loss: 0.5662 - val_accuracy: 0.7090\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 18s 174ms/step - loss: 0.6069 - accuracy: 0.6640 - val_loss: 0.5575 - val_accuracy: 0.7280\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 19s 193ms/step - loss: 0.6002 - accuracy: 0.6685 - val_loss: 0.5592 - val_accuracy: 0.7020\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 20s 197ms/step - loss: 0.5894 - accuracy: 0.6800 - val_loss: 0.5512 - val_accuracy: 0.7360\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 0.5956 - accuracy: 0.6750 - val_loss: 0.5526 - val_accuracy: 0.7100\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 0.5821 - accuracy: 0.6800 - val_loss: 0.5708 - val_accuracy: 0.7200\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 0.5745 - accuracy: 0.6945 - val_loss: 0.5460 - val_accuracy: 0.7250\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 0.5683 - accuracy: 0.7040 - val_loss: 0.5809 - val_accuracy: 0.6990\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 18s 174ms/step - loss: 0.5723 - accuracy: 0.6875 - val_loss: 0.5495 - val_accuracy: 0.6970\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 19s 194ms/step - loss: 0.5599 - accuracy: 0.7155 - val_loss: 0.5232 - val_accuracy: 0.7240\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 20s 195ms/step - loss: 0.5714 - accuracy: 0.6960 - val_loss: 0.5388 - val_accuracy: 0.7250\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 18s 184ms/step - loss: 0.5557 - accuracy: 0.7195 - val_loss: 0.5536 - val_accuracy: 0.7130\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 18s 177ms/step - loss: 0.5563 - accuracy: 0.7165 - val_loss: 0.5117 - val_accuracy: 0.7410\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 18s 175ms/step - loss: 0.5456 - accuracy: 0.7155 - val_loss: 0.5226 - val_accuracy: 0.7320\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 17s 174ms/step - loss: 0.5441 - accuracy: 0.7240 - val_loss: 0.5571 - val_accuracy: 0.6970\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 17s 171ms/step - loss: 0.5442 - accuracy: 0.7180 - val_loss: 0.5057 - val_accuracy: 0.7480\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 18s 180ms/step - loss: 0.5380 - accuracy: 0.7295 - val_loss: 0.5031 - val_accuracy: 0.7390\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 19s 191ms/step - loss: 0.5443 - accuracy: 0.7245 - val_loss: 0.5217 - val_accuracy: 0.7200\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 18s 176ms/step - loss: 0.5327 - accuracy: 0.7480 - val_loss: 0.5540 - val_accuracy: 0.7250\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 17s 167ms/step - loss: 0.5316 - accuracy: 0.7265 - val_loss: 0.4984 - val_accuracy: 0.7530\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 17s 168ms/step - loss: 0.5390 - accuracy: 0.7365 - val_loss: 0.5019 - val_accuracy: 0.7380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilham\\miniconda3\\envs\\vision1\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# The code below is to save your model as a .h5 file.\n",
    "# It will be saved automatically in your Submission folder.\n",
    "if __name__ == '__main__':\n",
    "    # DO NOT CHANGE THIS CODE\n",
    "    model = solution_C3()\n",
    "    model.save(\"model_C3.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
